{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('spark.app.name', 'TestCube'), ('spark.master', 'local[*]'), ('spark.executor.memory', '4g'), ('spark.executor.cores', '4'), ('spark.driver.memory', '32G'), ('spark.cores.max', '32'), ('spark.driver.maxResultSize', '10G'), ('spark.logConf', 'True')])\n",
      "CONTEXT INFO :  2.2.0.2.6.3.0-235 32G\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext, HiveContext, Row\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "def create_sc(app_name):\n",
    "    sc_conf = SparkConf()\n",
    "    sc_conf.setAppName(app_name)\n",
    "    sc_conf.setMaster('local[*]')\n",
    "    sc_conf.set('spark.executor.memory', '4g')\n",
    "    sc_conf.set('spark.executor.cores', '4')\n",
    "    sc_conf.set('spark.driver.memory', '32G')\n",
    "    sc_conf.set('spark.cores.max', '32')\n",
    "    sc_conf.set('spark.driver.maxResultSize', '10G')\n",
    "    sc_conf.set('spark.logConf', True)\n",
    "    print(sc_conf.getAll())\n",
    "\n",
    "    sc = None\n",
    "    try:\n",
    "        sc.stop()\n",
    "        sc = SparkContext(conf=sc_conf)\n",
    "    except:\n",
    "        sc = SparkContext(conf=sc_conf)\n",
    "\n",
    "    return sc\n",
    "\n",
    "sc = create_sc(\"TestCube\")\n",
    "print('CONTEXT INFO : ',sc.version,sc._conf.get('spark.driver.memory'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import time\n",
    "from struct import unpack_from\n",
    "\n",
    "dataFolder = '/user/gratienj/Data/TIM'\n",
    "filename = os.path.join(dataFolder,'binary_file4')\n",
    "t0 = time.process_time()\n",
    "record_length = 1\n",
    "binary_rdd = sc.binaryRecords(filename, record_length)\n",
    "# map()s each binary record to unpack() it\n",
    "unpacked_rdd = binary_rdd.map(lambda record: unpack_from('b', record))\n",
    "raw_data = unpacked_rdd.collect()\n",
    "t1=time.process_time()-t0\n",
    "print(\"HDFS READING TIME : \",t1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "localDataFolder='/home/gratienj/Data/TIM'\n",
    "local_filename=os.path.join(localDataFolder,'binary_file4')\n",
    "\n",
    "t0 = time.process_time()\n",
    "with open(local_filename, mode='rb') as file:\n",
    "    bytes_data = file.read()\n",
    "    array_data = np.frombuffer(bytes_data,dtype='b',count=bytes_data.__len__())\n",
    "t1=time.process_time()-t0\n",
    "print(\"Local FS READING TIME : \",t1)\n",
    "print('NB PIXELS :',len(array_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_length=1\n",
    "t0 = time.process_time()\n",
    "binary_rdd = sc.binaryRecords(filename, record_length)\n",
    "# map()s each binary record to unpack() it\n",
    "unpacked_rdd = binary_rdd.map(lambda record: unpack_from('1b', record)).flatMap(lambda x:x)\n",
    "raw_data = unpacked_rdd.collect()\n",
    "t1=time.process_time()-t0\n",
    "print(\"HDFS READING TIME : \",t1)\n",
    "print('NB PIXELS :',len(raw_data))\n",
    "\n",
    "record_length=2\n",
    "t0 = time.process_time()\n",
    "binary_rdd = sc.binaryRecords(filename, record_length)\n",
    "# map()s each binary record to unpack() it\n",
    "unpacked_rdd = binary_rdd.map(lambda record: unpack_from('2b', record)).flatMap(lambda x:x)\n",
    "raw_data = unpacked_rdd.collect()\n",
    "t1=time.process_time()-t0\n",
    "print(\"HDFS READING TIME : \",t1)\n",
    "print('NB PIXELS :',len(raw_data))\n",
    "record_length=4\n",
    "t0 = time.process_time()\n",
    "binary_rdd = sc.binaryRecords(filename, record_length)\n",
    "# map()s each binary record to unpack() it\n",
    "unpacked_rdd = binary_rdd.map(lambda record: unpack_from('4b', record)).flatMap(lambda x:x)\n",
    "raw_data = unpacked_rdd.collect()\n",
    "t1=time.process_time()-t0\n",
    "print(\"HDFS READING TIME : \",t1)\n",
    "print('NB PIXELS :',len(raw_data))\n",
    "record_length=8\n",
    "t0 = time.process_time()\n",
    "binary_rdd = sc.binaryRecords(filename, record_length)\n",
    "# map()s each binary record to unpack() it\n",
    "unpacked_rdd = binary_rdd.map(lambda record: unpack_from('8b', record)).flatMap(lambda x:x)\n",
    "raw_data = unpacked_rdd.collect()\n",
    "t1=time.process_time()-t0\n",
    "print(\"HDFS READING TIME : \",t1)\n",
    "print('NB PIXELS :',len(raw_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST MONGODB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import gridfs\n",
    "from pymongo import MongoClient\n",
    "\n",
    "client = MongoClient('mongodb://%s:%s@islin-hdpnod1.ifp.fr' % (\"gratienj\", \"gratienj2019!\"))\n",
    "\n",
    "#\n",
    "# GETTING a Database\n",
    "db = client['tim-cube']\n",
    "grid_fs = gridfs.GridFS(db,'bdata')\n",
    "\n",
    "# GETTING a collection\n",
    "cube_collection = db['cube-collection1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cube(nx,ny,nz):\n",
    "    random_array = np.random.randint(0,255,size=(nx,ny,nz))\n",
    "    return np.array(random_array,dtype=np.uint8)\n",
    "\n",
    "def writeCubeToDataBase(idx,cube,nx,ny,nz,collection,gfs):\n",
    "        bytes_data = cube_data.tobytes()\n",
    "        img_data_id = gfs.put(bytes_data)\n",
    "        name=\"cube-\"+str(nx)+\"x\"+str(ny)+\"x\"+str(nz)\n",
    "        img_doc = {\"name\":name,'id':idx,'data_id':img_data_id,'shape':{'nx':nx,'ny':ny,'nz':nz}}\n",
    "        img_id = collection.insert_one(img_doc).inserted_id\n",
    "        print('add to DB IMG DOC : ',name,\" id=\",img_id)    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes=[10,20,50,100,200,400,800]\n",
    "idx=0\n",
    "for nx in sizes:\n",
    "    print(\"CUBE : \",nx)\n",
    "    cube_data = create_cube(nx,nx,nx)\n",
    "    t0 = time.process_time()\n",
    "    writeCubeToDataBase(idx,cube_data,nx,nx,nx,cube_collection,grid_fs)\n",
    "    t1=time.process_time()-t0\n",
    "    print(\"MONGODB WRITTING TIME : \",t1)\n",
    "    idx=idx+1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCubeFromDataBase(name,collection,gfs):\n",
    "    #import pprint\n",
    "    cube = collection.find_one({\"name\":name})\n",
    "    #pprint.pprint(img)\n",
    "    print('ID:',cube['id'])\n",
    "    shape=cube['shape']\n",
    "    #print('SHAPE',shape['nx'],shape['ny'],shape['nz'])\n",
    "    data_file = gfs.find_one({\"_id\":cube['data_id']})\n",
    "    bytes_data = data_file.read()\n",
    "    print('NB BYTES',bytes_data.__len__(),np.dtype('uint8').itemsize)\n",
    "    array_data = np.frombuffer(bytes_data,dtype='uint8',count=int(bytes_data.__len__()/np.dtype('uint8').itemsize))\n",
    "    array3D_data =np.reshape(array_data,(shape['nx'],shape['ny'],shape['nz']))\n",
    "    return array3D_data\n",
    "\n",
    "def deleteCubeFrmDataBase(name,collection,gfs):\n",
    "    for cube in collection.find({\"name\":name}):\n",
    "        print('ID:',cube['id'])\n",
    "        shape=cube['shape']\n",
    "        #print('SHAPE',shape['nx'],shape['ny'],shape['nz'])\n",
    "        gfs.delete_many({\"_id\":cube['data_id']})\n",
    "        collection.delete_one({\"_id\":cube['_id']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes=[10,20,50,100,200,400,800]\n",
    "for nx in sizes:\n",
    "    print(\"CUBE : \",nx)\n",
    "    name=\"cube-\"+str(nx)+\"x\"+str(nx)+\"x\"+str(nx)\n",
    "    cube_data=getCubeFromDataBase(name,cube_collection,grid_fs)\n",
    "print(\"COLLECTION SIZE\",cube_collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid_fs.delete_many({})\n",
    "cube_collection.delete_many({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes=[10,20,50,100,200,400,800]\n",
    "idx=0\n",
    "for nx in sizes:\n",
    "    print(\"CUBE : \",nx)\n",
    "    name=\"cube-\"+str(nx)+\"x\"+str(nx)+\"x\"+str(nx)\n",
    "    t0 = time.process_time()\n",
    "    cube_data=getCubeFromDataBase(name,cube_collection,grid_fs)\n",
    "    t1=time.process_time()-t0\n",
    "    print(\"MONGODB READ TIME : \",t1)\n",
    "    print('SHAPE :',cube_data.shape)\n",
    "    idx=idx+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
