{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TimeSeries DataAnalytics Tutorial\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import chart_studio.plotly as py\n",
    "import plotly.express as px\n",
    "import plotly.tools as tls\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import json\n",
    "#import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName=\"TimeSeries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I/ Simple IRIS example to check PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "data_dir = '/work/irlin355_1/gratienj/BigData/DataLakeBenchProject/python/TimeSeries/DataAnalytics'\n",
    "filename = os.path.join(data_dir,'iris.csv')\n",
    "df = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SQLContext\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, StringType\n",
    "sqlContext = SQLContext(sc)\n",
    "schema = StructType([StructField(\"sepal_length\", DoubleType(), True),\n",
    "                     StructField(\"sepal_width\",  DoubleType(), True),\n",
    "                     StructField(\"petal_length\", DoubleType(), True),\n",
    "                     StructField(\"petal_width\",  DoubleType(), True),\n",
    "                     StructField(\"variety\",      StringType(), True),\n",
    "                    ])\n",
    "spark_df = sqlContext.createDataFrame(df,schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I/ SmartGrid Example from File\n",
    "\n",
    "### A/ Standard method with Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_timestamp(date):\n",
    "    dt = datetime.strptime(date, '%d/%m/%Y %H:%M:%S')\n",
    "    d0 = datetime(2019,1,1,0,0,0,0)\n",
    "    return int((dt - d0).total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_curve(df,day_id,tagname):\n",
    "    day_df = df.loc[(df['day_id'] == day_id) & (df['tagname'] == tagname )]\n",
    "    vh_df = day_df[['hour_id','value']].groupby('hour_id').mean().reset_index().sort_values(by='hour_id')\n",
    "    x = vh_df['hour_id']\n",
    "    y = vh_df['value']\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_dir = '/work/irlin355_1/gratienj/BigData/DigitalSandBox/Data/TimeSeries/SmartGridData/Cryolite/20190101'\n",
    "filename = os.path.join(data_dir,'OneMonth.csv')\n",
    "df = pd.read_csv(filename, sep=';')\n",
    "#tagnames = ['CRY.CENTRALE_SOLAIRE.CRY_act_prod_pow']\n",
    "tagnames = df.tagname.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.4 s, sys: 0 ns, total: 17.4 s\n",
      "Wall time: 17.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['value'] = pd.to_numeric(df['value'])\n",
    "df['quality'] = pd.to_numeric(df['value'])\n",
    "df['timestamp'] = df['timestamp'].apply(str_to_timestamp)\n",
    "df['day_id'] = df['timestamp']//(3600*24)\n",
    "df['hour_id'] = df['timestamp'] % (3600*24) // 3600\n",
    "results =[]\n",
    "for tag_id,tagname in enumerate(tagnames):\n",
    "    for day_id in range(30):\n",
    "        results.append(compute_curve(df,day_id,tagname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B/ PySpark method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_dir = '/work/irlin355_1/gratienj/BigData/DigitalSandBox/Data/TimeSeries/SmartGridData/Cryolite/20190101'\n",
    "filename = os.path.join(data_dir,'OneMonth.csv')\n",
    "df = pd.read_csv(filename, sep=';')\n",
    "df['value'] = pd.to_numeric(df['value'])\n",
    "df['quality'] = pd.to_numeric(df['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SQLContext\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, StringType\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Schema = StructType([ StructField(\"timestamp\", StringType(),    True),\n",
    "                      StructField(\"tagname\",   StringType(),  True),\n",
    "                      StructField(\"value\",     DoubleType(),  True),\n",
    "                      StructField(\"quality\",   DoubleType(),  True)\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = sqlContext.createDataFrame(df,schema=Schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- tagname: string (nullable = true)\n",
      " |-- value: double (nullable = true)\n",
      " |-- quality: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+-----+-------+\n",
      "|          timestamp|             tagname|value|quality|\n",
      "+-------------------+--------------------+-----+-------+\n",
      "|01/01/2019 09:15:12|CRY.CENTRALE_SOLA...|  1.0|    1.0|\n",
      "|01/01/2019 09:15:18|CRY.CENTRALE_SOLA...|  0.0|    0.0|\n",
      "|01/01/2019 09:15:37|CRY.CENTRALE_SOLA...|  1.0|    1.0|\n",
      "|01/01/2019 09:15:43|CRY.CENTRALE_SOLA...|  0.0|    0.0|\n",
      "|01/01/2019 09:15:53|CRY.CENTRALE_SOLA...|  1.0|    1.0|\n",
      "|01/01/2019 09:15:58|CRY.CENTRALE_SOLA...|  0.0|    0.0|\n",
      "|01/01/2019 09:16:13|CRY.CENTRALE_SOLA...|  1.0|    1.0|\n",
      "|01/01/2019 09:16:23|CRY.CENTRALE_SOLA...|  0.0|    0.0|\n",
      "|01/01/2019 09:16:53|CRY.CENTRALE_SOLA...|  1.0|    1.0|\n",
      "|01/01/2019 09:16:59|CRY.CENTRALE_SOLA...|  0.0|    0.0|\n",
      "|01/01/2019 09:17:03|CRY.CENTRALE_SOLA...|  1.0|    1.0|\n",
      "|01/01/2019 09:17:09|CRY.CENTRALE_SOLA...|  0.0|    0.0|\n",
      "|01/01/2019 09:17:18|CRY.CENTRALE_SOLA...|  1.0|    1.0|\n",
      "|01/01/2019 09:17:24|CRY.CENTRALE_SOLA...|  0.0|    0.0|\n",
      "|01/01/2019 09:17:29|CRY.CENTRALE_SOLA...|  1.0|    1.0|\n",
      "|01/01/2019 09:17:34|CRY.CENTRALE_SOLA...|  0.0|    0.0|\n",
      "|01/01/2019 09:17:39|CRY.CENTRALE_SOLA...|  1.0|    1.0|\n",
      "|01/01/2019 09:17:44|CRY.CENTRALE_SOLA...|  0.0|    0.0|\n",
      "|01/01/2019 09:17:55|CRY.CENTRALE_SOLA...|  1.0|    1.0|\n",
      "|01/01/2019 09:17:59|CRY.CENTRALE_SOLA...|  0.0|    0.0|\n",
      "+-------------------+--------------------+-----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_timestamp(date):\n",
    "    dt = datetime.strptime(date, '%d/%m/%Y %H:%M:%S')\n",
    "    d0 = datetime(2019,1,1,0,0,0,0)\n",
    "    return int((dt - d0).total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, StringType, LongType\n",
    "F1 = udf(lambda x: str_to_timestamp(x), LongType())\n",
    "dayId = udf(lambda x : x//(3600*24))\n",
    "hourId = udf(lambda x : x % (3600*24) // 3600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_curve(tag_df,day_id):\n",
    "    day_df = tag_df.filter(tag_df.day_id == day_id)\n",
    "    rdd = day_df.select('hour_id','value').groupby('hour_id').mean()\n",
    "    x = [x[\"hour_id\"] for x in rdd.select(\"hour_id\").collect()]\n",
    "    y = [x[\"avg(value)\"] for x in rdd.select(\"avg(value)\").collect()]\n",
    "    return sorted(zip(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.75 s, sys: 1.81 s, total: 6.56 s\n",
      "Wall time: 10min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tagnames = df.tagname.unique()\n",
    "spark_df2 = spark_df.withColumn(\"timestamp_sec\",  F1(spark_df[\"timestamp\"]))\n",
    "spark_df3 = spark_df2.withColumn(\"day_id\",dayId(spark_df2['timestamp_sec']))\n",
    "spark_df4 = spark_df3.withColumn(\"hour_id\",hourId(spark_df3['timestamp_sec']))\n",
    "results =[]\n",
    "for tag_id,tagname in enumerate(tagnames):\n",
    "    tag_df = spark_df4.filter(spark_df4.tagname == tagname)\n",
    "    for day_id in range(30):\n",
    "        results.append(compute_curve(tag_df,day_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C/ Data from DataBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to connect to MongoDB server: localhost on port: 27018\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../TimeSeriesTools\")\n",
    "import mongodb_utils\n",
    "db_host= 'localhost'\n",
    "port = '27018'\n",
    "db_name='TimeSeriesBench'\n",
    "mongodb_client = mongodb_utils.mongodb_connect(db_host, port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_collection_scheme(db_name,scheme_name):\n",
    "    db = mongodb_client[db_name]\n",
    "    schemes_coll = db['schemes']\n",
    "    scheme = schemes_coll.find({\"name\":scheme_name})\n",
    "    return scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mongodb_find_all_data(db_name,coll_name,scheme):\n",
    "    data = mongodb_utils.get_all_data(mongodb_client,db_name,coll_name,scheme)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheme = get_collection_scheme(db_name,'SmartGrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "769598  documents found\n",
      "number of docs 769598\n",
      "CPU times: user 4.5 s, sys: 7.99 ms, total: 4.51 s\n",
      "Wall time: 4.81 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "coll_name='SmartGridCryolite20190101OneMonthBS10000d'\n",
    "data = mongodb_find_all_data(db_name,coll_name,scheme[0]['value'])\n",
    "print(\"number of docs\",len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,d in enumerate(data):\n",
    "    d['_id'] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.05 s, sys: 0 ns, total: 1.05 s\n",
      "Wall time: 1.05 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>tagname</th>\n",
       "      <th>value</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>01/01/2019 09:15:12</td>\n",
       "      <td>CRY.CENTRALE_SOLAIRE.CRY_act_prod_pow</td>\n",
       "      <td>1.000000000</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>01/01/2019 09:15:18</td>\n",
       "      <td>CRY.CENTRALE_SOLAIRE.CRY_act_prod_pow</td>\n",
       "      <td>0.000000000</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>01/01/2019 09:15:37</td>\n",
       "      <td>CRY.CENTRALE_SOLAIRE.CRY_act_prod_pow</td>\n",
       "      <td>1.000000000</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>01/01/2019 09:15:43</td>\n",
       "      <td>CRY.CENTRALE_SOLAIRE.CRY_act_prod_pow</td>\n",
       "      <td>0.000000000</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>01/01/2019 09:15:53</td>\n",
       "      <td>CRY.CENTRALE_SOLAIRE.CRY_act_prod_pow</td>\n",
       "      <td>1.000000000</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   _id            timestamp                                tagname  \\\n",
       "0    0  01/01/2019 09:15:12  CRY.CENTRALE_SOLAIRE.CRY_act_prod_pow   \n",
       "1    1  01/01/2019 09:15:18  CRY.CENTRALE_SOLAIRE.CRY_act_prod_pow   \n",
       "2    2  01/01/2019 09:15:37  CRY.CENTRALE_SOLAIRE.CRY_act_prod_pow   \n",
       "3    3  01/01/2019 09:15:43  CRY.CENTRALE_SOLAIRE.CRY_act_prod_pow   \n",
       "4    4  01/01/2019 09:15:53  CRY.CENTRALE_SOLAIRE.CRY_act_prod_pow   \n",
       "\n",
       "         value quality  \n",
       "0  1.000000000   100.0  \n",
       "1  0.000000000   100.0  \n",
       "2  1.000000000   100.0  \n",
       "3  0.000000000   100.0  \n",
       "4  1.000000000   100.0  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df = pd.DataFrame(data)\n",
    "df[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "Schema = StructType([ StructField(\"_id\",       LongType(),  True),\n",
    "                      StructField(\"timestamp\", StringType(),    True),\n",
    "                      StructField(\"tagname\",   StringType(),  True),\n",
    "                      StructField(\"value\",     StringType(),  True),\n",
    "                      StructField(\"quality\",   StringType(),  True)\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = sqlContext.createDataFrame(df,schema=Schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "indexer = StringIndexer(inputCol=\"tagname\", outputCol=\"tagindex\")\n",
    "indexed_df = indexer.fit(spark_df).transform(spark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagnames = df.tagname.unique()\n",
    "max_tag_index = len(tagnames)+1\n",
    "dayId = udf(lambda x : x//(3600*24))\n",
    "hourId = udf(lambda x,y : int(y) * 24 + (x % (3600*24) // 3600 )  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 42.5 ms, sys: 0 ns, total: 42.5 ms\n",
      "Wall time: 8.55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "spark_df1 = indexed_df.withColumn(\"timestamp_sec\",  F1(indexed_df[\"timestamp\"]))\n",
    "spark_df2 = spark_df1.withColumn(\"value_d\",  spark_df1[\"value\"].cast(\"double\"))\n",
    "spark_df3 = spark_df2.withColumn(\"day_id\",dayId(spark_df2['timestamp_sec']))\n",
    "spark_df4 = spark_df3.withColumn(\"hour_id\",hourId(spark_df3['timestamp_sec'],spark_df3['tagindex']))\n",
    "rdd = spark_df4.select('hour_id','value_d').groupby('hour_id').mean()\n",
    "x = [x[\"hour_id\"] for x in rdd.select(\"hour_id\").collect()]\n",
    "y = [x[\"avg(value_d)\"] for x in rdd.select(\"avg(value_d)\").collect()]\n",
    "result = sorted(zip(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kernel_ts_mongodb_pyspark",
   "language": "python",
   "name": "kernel_ts_mongodb_pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
