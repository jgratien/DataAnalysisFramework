{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kafka Data Consumer\n",
    "\n",
    "## I/ Objectifs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Collecte en streaming des données \n",
    "- Nettoyage des données\n",
    "- Restruction des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II/ Mécanisme de collecte en streaming des données\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "sys.path.append(\"../TimeSeriesTools\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_server ='localhost:9092'\n",
    "#kafka_server ='islin-hdpnod1:6667'\n",
    "#kafka_server ='islin-hdplnod06:6667'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kairosdb_utils\n",
    "global kairosdb_server \n",
    "kairosdb_server = \"http://localhost:6060\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to connect to InfluxDB server without proxy: localhost on port: 9086\n",
      "connection sucess!\n"
     ]
    }
   ],
   "source": [
    "import influxdb_utils\n",
    "db_host= 'localhost'\n",
    "port = '9086'\n",
    "db_name='TimeSeriesBench'\n",
    "influxdb_client = influxdb_utils.influxdb_connect(db_host, port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warp10_utils\n",
    "global warp10_server \n",
    "warp10_server = \"http://localhost:7070\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer, TopicPartition\n",
    "def consumer_single_collection(kafka_server,topic,db_name, coll_name,bucket_size,insert_data,convert_to_dbdata,validate_data):\n",
    "    print(\"CONNECTION KAFKA : \",kafka_server)\n",
    "    print(\"TOPIC            : \",topic)\n",
    "    print(\"DB NAME          : \",db_name)\n",
    "    print(\"COLLECTION NAME  : \",coll_name)\n",
    "    \n",
    "    scheme = get_scheme(topic)\n",
    "    consumer = KafkaConsumer(bootstrap_servers=kafka_server)\n",
    "\n",
    "    tp = TopicPartition(topic, 0)\n",
    "    consumer.assign([tp])\n",
    "    consumer.seek_to_end(tp)\n",
    "    lastOffset = consumer.position(tp)\n",
    "    print(\"RANGE:\",tp,lastOffset)\n",
    "    consumer.seek_to_beginning(tp)\n",
    "    # end\n",
    "    try :\n",
    "        while True:\n",
    "            list_data = []\n",
    "            count=0\n",
    "            for message in consumer:\n",
    "                msg = message.value.decode()\n",
    "                count += 1\n",
    "                list_data.append(msg)\n",
    "                if len(list_data) == bucket_size:\n",
    "                    #print(count,str(len(list_data)) + \" data fetched from topic : \" + topic)\n",
    "                    new_data = convert_to_dbdata(list_data,scheme,validate_data)\n",
    "                    insert_data(db_name, coll_name, new_data)\n",
    "                    list_data.clear()\n",
    "                if count == lastOffset:\n",
    "                    #print(count,str(len(list_data)) + \" data fetched from topic : \" + topic)\n",
    "                    new_data = convert_to_dbdata(list_data,scheme,validate_data)\n",
    "                    insert_data(db_name, coll_name, new_data)\n",
    "                    list_data.clear()\n",
    "                    return\n",
    "\n",
    "    finally:\n",
    "        # Close down consumer to commit final offsets.\n",
    "        consumer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III/ Mécanismes de validation et corrections des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scheme(topic):\n",
    "    if \"SmartGrid\" in topic:\n",
    "        scheme = {\"timestamp\":{'type': 'string','required': True,'empty': False},\n",
    "                  \"tagname\":{'type': 'string','required': True,'empty': False},\n",
    "                  \"value\":{'type': 'string','required': True,'empty': False},\n",
    "                  \"quality\":{'type': 'string','required': True,'empty': False}}\n",
    "        return scheme\n",
    "    if \"WindProp\" in topic :\n",
    "        scheme = {  \"Heure\":{'type': 'string','required': True,'empty': False},\n",
    "                    \"Temps écoulé\":{'type': 'string','required': True,'empty': False},\n",
    "                    '4069 state':{'type': 'string','required': True,'empty': False},\n",
    "                    'Battery voltage':{'type': 'string','required': True,'empty': False},\n",
    "                    'Flow SP':{'type': 'string','required': True,'empty': False},\n",
    "                    'Unit Code':{'type': 'string','required': True,'empty': False},\n",
    "                    'Flow M':{'type': 'string','required': True,'empty': False},\n",
    "                    'Pressure1':{'type': 'string','required': True,'empty': False},\n",
    "                    'Temperature1':{'type': 'string','required': True,'empty': False},\n",
    "                    \"Latitude\":{'type': 'string','required': True,'empty': False},\n",
    "                    \"Longitude\":{'type': 'string','required': True,'empty': False},\n",
    "                    \"Altitude\":{'type': 'string','required': True,'empty': False},\n",
    "                    \"Head_Rel_True North\":{'type': 'string','required': True,'empty': False},\n",
    "                    \"Pressure2\":{'type': 'string','required': True,'empty': False},\n",
    "                    \"Temperature2\":{'type': 'string','required': True,'empty': False},\n",
    "                    \"Humidity\":{'type': 'string','required': True,'empty': False},\n",
    "                    \"MDA Wnd Dir\":{'type': 'string','required': True,'empty': False},\n",
    "                    \"MDA Wnd Speed\":{'type': 'string','required': True,'empty': False},\n",
    "                    \"MWD Wind Dir\":{'type': 'string','required': True,'empty': False},\n",
    "                    \"MWD Wind Speed\":{'type': 'string','required': True,'empty': False},\n",
    "                    'Gaz Concentration 1':{'type': 'string','required': True,'empty': False},\n",
    "                    'Gaz Concentration 2':{'type': 'string','required': True,'empty': False},\n",
    "                    'Gaz Concentration 3':{'type': 'string','required': True,'empty': False},\n",
    "                    'Gaz Concentration 4':{'type': 'string','required': True,'empty': False},\n",
    "                    'SPA 1':{'type': 'string','required': True,'empty': False},\n",
    "                    'SPA 2':{'type': 'string','required': True,'empty': False},\n",
    "                    'Cellule Photo':{'type': 'string','required': True,'empty': False},\n",
    "                    'Temperature3':{'type': 'string','required': True,'empty': False},\n",
    "                    'Pressure3':{'type': 'string','required': True,'empty': False},\n",
    "                    'Flow MassFlow 1':{'type': 'string','required': True,'empty': False},\n",
    "                    'NOTUSED Flow MassFlow 2':{'type': 'string','required': True,'empty': False},\n",
    "                    'Flow':{'type': 'string','required': True,'empty': False},\n",
    "                    'Humidity2':{'type': 'string','required': True,'empty': False},\n",
    "                    'Test':{'type': 'string','required': True,'empty': False},\n",
    "                    'Details':{'type': 'string','required': True,'empty': False},\n",
    "                    'SPA 3':{'type': 'string','required': True,'empty': False},\n",
    "                    'SPA 4':{'type': 'string','required': True,'empty': False},\n",
    "                    \"CavityPressure\":{'type': 'string','required': True,'empty': False},\n",
    "                    \"CavityTemp\":{'type': 'string','required': True,'empty': False},\n",
    "                    \"CH4\":{'type': 'string','required': True,'empty': False},\n",
    "                    \"CH4_dry\":{'type': 'string','required': True,'empty': False},\n",
    "                    \"C2H6\":{'type': 'string','required': True,'empty': False},\n",
    "                    \"C2H6_dry\":{'type': 'string','required': True,'empty': False},\n",
    "                    \"13CH4\":{'type': 'string','required': True,'empty': False},\n",
    "                    \"H2O\":{'type': 'string','required': True,'empty': False},\n",
    "                    \"CO2\":{'type': 'string','required': True,'empty': False},\n",
    "                    \"C2C1Ratio\":{'type': 'string','required': True,'empty': False},\n",
    "                    \"Delta_iCH4_Raw\":{'type': 'string','required': True,'empty': False},\n",
    "                    \"HP_Delta_iCH4_30s\":{'type': 'string','required': True,'empty': False},\n",
    "                    \"HP_Delta_iCH4_2min\":{'type': 'string','required': True,'empty': False},\n",
    "                    \"HP_Delta_iCH4_5min\":{'type': 'string','required': True,'empty': False} }\n",
    "        return scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data,scheme):\n",
    "    from cerberus import Validator\n",
    "    v = Validator(scheme)\n",
    "    for index,item in enumerate(data,start=0):\n",
    "        res = v.validate(item)\n",
    "        if (res == False):\n",
    "            print(\"corrupt data in line :\",index,\", error : \",v.errors)\n",
    "            del data[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV/ Mécanismes de mise en base des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to connect to MongoDB server: localhost on port: 28018\n"
     ]
    }
   ],
   "source": [
    "import mongodb_utils\n",
    "db_host= 'localhost'\n",
    "port = '28018'\n",
    "db_name='TimeSeriesBench'\n",
    "mongodb_client = mongodb_utils.mongodb_connect(db_host, port)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A/ Mise en base du schéma des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_unix(date):\n",
    "    from datetime import datetime\n",
    "    dt = datetime.strptime(date, '%d/%m/%Y %H:%M:%S')\n",
    "    epoch = datetime.utcfromtimestamp(0)\n",
    "    return int((dt - epoch).total_seconds()) * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def type_convert(data):\n",
    "    data = data.replace(',', '.')\n",
    "    time = str_to_unix(data.split(\";\", 1)[0].split(\":\", 1)[1])\n",
    "    tags = data.split(\";\", 2)[1]\n",
    "    data_treated = [tags,[time, data.split(\";\", 2)[2]]]\n",
    "    return data_treated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mongodb_convert_to_dbdata(list_data,scheme,validate_data):\n",
    "    col= [k for k in scheme.keys()]\n",
    "    new_dict=[]\n",
    "    \n",
    "    t0 = time.process_time()\n",
    "    for item in list_data:\n",
    "        info=item.split(\":\",1)[1].split(\";\")\n",
    "        dict_item ={col[i]:info[i] for i in range(len(col))}\n",
    "        new_dict.append(dict_item)\n",
    "    if validate_data:\n",
    "        clean_data(new_dict,scheme)\n",
    "    t1 = time.process_time()\n",
    "    #print('.. %f seconds for validate' % (t1 - t0))\n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def db_convert_to_dbdata(list_msg,scheme,validate_data):\n",
    "    from cerberus import Validator\n",
    "    col = [k for k in scheme.keys()]\n",
    "    if validate_data:\n",
    "        v = Validator(scheme)\n",
    "    db_data = []\n",
    "    t0 = time.process_time()\n",
    "    for index,item in enumerate(list_msg):\n",
    "        if validate_data:\n",
    "            item_1 = item.replace(',', '.')\n",
    "            info = item_1.split(\":\", 1)[1].split(\";\")\n",
    "            dict_item = {col[i]: info[i] for i in range(len(col))}\n",
    "            res = v.validate(dict_item)\n",
    "        else:\n",
    "            res = True\n",
    "        if res == True:\n",
    "            new_data = type_convert(item)\n",
    "            db_data.append(new_data)\n",
    "        else:\n",
    "            print(\"corrupt data in line :\",index,\", error : \",v.errors)\n",
    "    t1 = time.process_time()\n",
    "    #print('.. %f seconds for validate' % (t1 - t0))\n",
    "    return db_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mongodb_insert_data(db_name, coll_name, list_data):\n",
    "    mongodb_utils.insert_many_docs(mongodb_client,db_name, coll_name,list_data)    \n",
    "    return len(list_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kairosdb_insert_data(db_name, coll_name, list_data):\n",
    "    tags_data_list = {}\n",
    "    for d in list_data:\n",
    "        if d[0] in tags_data_list.keys():\n",
    "            tags_data_list[d[0]].append(d[1])\n",
    "        else:\n",
    "            tags_data_list[d[0]] = [d[1]]\n",
    "    \n",
    "    tag_keys = ['Buiding','Device','Measure']\n",
    "    for i,(k,v) in enumerate(tags_data_list.items()):\n",
    "        tag_values = k.split(\".\")\n",
    "        tags = {tag_keys[i]:tag_values[i] for i in range(len(tag_keys))}\n",
    "        kairosdb_utils.insert_many_docs_with_tags(kairosdb_server,db_name, coll_name,tags,v)    \n",
    "    return len(list_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def influxdb_insert_data(db_name, coll_name, list_data):\n",
    "    tags_data_list = {}\n",
    "    for d in list_data:\n",
    "        if d[0] in tags_data_list.keys():\n",
    "            tags_data_list[d[0]].append(d[1])\n",
    "        else:\n",
    "            tags_data_list[d[0]] = [d[1]]\n",
    "    \n",
    "    tag_keys = ['Buiding','Device','Measure']\n",
    "    for i,(k,v) in enumerate(tags_data_list.items()):\n",
    "        tag_values = k.split(\".\")\n",
    "        tags = {tag_keys[i]:tag_values[i] for i in range(len(tag_keys))}\n",
    "        influxdb_utils.insert_many_docs_with_tags(influxdb_client,db_name, coll_name,tags,v)    \n",
    "    return len(list_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warp10_insert_data(db_name, coll_name, list_data):\n",
    "    tags_data_list = {}\n",
    "    for d in list_data:\n",
    "        if d[0] in tags_data_list.keys():\n",
    "            tags_data_list[d[0]].append(d[1])\n",
    "        else:\n",
    "            tags_data_list[d[0]] = [d[1]]\n",
    "    \n",
    "    tag_keys = ['Buiding','Device','Measure']\n",
    "    for i,(k,v) in enumerate(tags_data_list.items()):\n",
    "        tag_values = k.split(\".\")\n",
    "        tags = {tag_keys[i]:tag_values[i] for i in range(len(tag_keys))}\n",
    "        warp10_utils.insert_many_docs_with_tags(warp10_server,db_name, coll_name,tags,v)    \n",
    "    return len(list_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Cas test SmartGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = 'SmartGridDataCryolite20190101OneMonthNew2Topic'\n",
    "scheme = get_scheme(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "coll_name='SmartGridCryolite20190101OneMonthV10000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SmartGridCryolite20190101OneMonthBS100000d',\n",
       " 'SmartGridCryolite20190101OneMonthBS10000d',\n",
       " 'SmartGridCryolite20190101OneDayBS1000d',\n",
       " 'SmartGridCryolite20190101OneDayBS10000d',\n",
       " 'SmartGridCryolite20190101OneMonthV10000',\n",
       " 'schemes',\n",
       " 'scheme_collection',\n",
       " 'SmartGridCryolite20190101OneMonthBS50000d',\n",
       " 'SmartGridCryolite20190101OneMonthBS1000d',\n",
       " 'WindPropLacqOneDayP25BS100',\n",
       " 'WindPropLacqOneDayP25-B1000',\n",
       " 'WindPropLacqOneDayP25BS1000']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db=mongodb_client[db_name]\n",
    "db.list_collection_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheme_collection = db['schemes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'timestamp': {'type': 'string', 'required': True, 'empty': False},\n",
       " 'tagname': {'type': 'string', 'required': True, 'empty': False},\n",
       " 'value': {'type': 'string', 'required': True, 'empty': False},\n",
       " 'quality': {'type': 'string', 'required': True, 'empty': False}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheme_doc = { \"name\" : \"SmartGrid\" , \"value\" : scheme }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheme_id = scheme_collection.insert_one(scheme_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertOneResult at 0x7f2ce62c04b0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scheme_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONNECTION KAFKA :  localhost:9092\n",
      "TOPIC            :  SmartGridDataCryolite20190101OneMonthNew2Topic\n",
      "DB NAME          :  TimeSeriesBench\n",
      "COLLECTION NAME  :  SmartGridCryolite20190101OneMonthV10000\n",
      "RANGE: TopicPartition(topic='SmartGridDataCryolite20190101OneMonthNew2Topic', partition=0) 769598\n",
      "CPU times: user 22.2 s, sys: 213 ms, total: 22.4 s\n",
      "Wall time: 27.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bucket_size = 10000\n",
    "validate_data = False\n",
    "consumer_single_collection(kafka_server,topic,db_name, coll_name,bucket_size,mongodb_insert_data,mongodb_convert_to_dbdata,validate_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONNECTION KAFKA :  localhost:9092\n",
      "TOPIC            :  SmartGridDataCryolite20190101OneMonthNew2Topic\n",
      "DB NAME          :  TimeSeriesBench\n",
      "COLLECTION NAME  :  SmartGridCryolite20190101OneMonthV10000\n",
      "RANGE: TopicPartition(topic='SmartGridDataCryolite20190101OneMonthNew2Topic', partition=0) 769598\n",
      "CPU times: user 24.9 s, sys: 325 ms, total: 25.2 s\n",
      "Wall time: 1min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bucket_size = 10000\n",
    "validate_data = False\n",
    "consumer_single_collection(kafka_server,topic,db_name, coll_name,bucket_size,kairosdb_insert_data,db_convert_to_dbdata,validate_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONNECTION KAFKA :  localhost:9092\n",
      "TOPIC            :  SmartGridDataCryolite20190101OneMonthNew2Topic\n",
      "DB NAME          :  TimeSeriesBench\n",
      "COLLECTION NAME  :  SmartGridCryolite20190101OneMonthV10000\n",
      "RANGE: TopicPartition(topic='SmartGridDataCryolite20190101OneMonthNew2Topic', partition=0) 769598\n",
      "CPU times: user 37.8 s, sys: 365 ms, total: 38.2 s\n",
      "Wall time: 45.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bucket_size = 10000\n",
    "validate_data = False\n",
    "consumer_single_collection(kafka_server,topic,db_name, coll_name,bucket_size,influxdb_insert_data,db_convert_to_dbdata,validate_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONNECTION KAFKA :  localhost:9092\n",
      "TOPIC            :  SmartGridDataCryolite20190101OneMonthNew2Topic\n",
      "DB NAME          :  TimeSeriesBench\n",
      "COLLECTION NAME  :  SmartGridCryolite20190101OneMonthV10000\n",
      "RANGE: TopicPartition(topic='SmartGridDataCryolite20190101OneMonthNew2Topic', partition=0) 769598\n",
      "CPU times: user 29.5 s, sys: 392 ms, total: 29.9 s\n",
      "Wall time: 43 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bucket_size = 10000\n",
    "validate_data = False\n",
    "consumer_single_collection(kafka_server,topic,db_name, coll_name,bucket_size,warp10_insert_data,db_convert_to_dbdata,validate_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Cas test WindProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def type_convert(data):\n",
    "    data = data.replace(',', '.')\n",
    "    time = str_to_unix(data.split(\";\", 1)[0].split(\":\", 1)[1])\n",
    "    data_treated = [time, data.split(\";\", 1)[1]]\n",
    "    return data_treated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def db_convert_to_dbdata(list_msg,scheme,validate_data):\n",
    "    from cerberus import Validator\n",
    "    col = [k for k in scheme.keys()]\n",
    "    if validate_data:\n",
    "        v = Validator(scheme)\n",
    "    db_data = []\n",
    "    t0 = time.process_time()\n",
    "    for index,item in enumerate(list_msg):\n",
    "        if validate_data:\n",
    "            info = item.split(\":\", 1)[1].split(\";\")\n",
    "            dict_item = {col[i]: info[i] for i in range(len(col))}\n",
    "            res = v.validate(dict_item)\n",
    "        else:\n",
    "            res = True\n",
    "        if res == True:\n",
    "            new_data = type_convert(item)\n",
    "            db_data.append(new_data)\n",
    "        else:\n",
    "            print(\"corrupt data in line :\",index,\", error : \",v.errors)\n",
    "    t1 = time.process_time()\n",
    "    #print('.. %f seconds for validate' % (t1 - t0))\n",
    "    return db_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kairosdb_insert_data(db_name, coll_name, list_data):\n",
    "    kairosdb_utils.insert_many_docs(kairosdb_server,db_name, coll_name,list_data)    \n",
    "    return len(list_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def influxdb_insert_data(db_name, coll_name, list_data):\n",
    "    influxdb_utils.insert_many_docs(influxdb_client,db_name, coll_name,list_data)    \n",
    "    return len(list_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warp10_insert_data(db_name, coll_name, list_data):\n",
    "    warp10_utils.insert_many_docs(warp10_server,db_name, coll_name,list_data)    \n",
    "    return len(list_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = 'WindPropDataLacqOneDayP25NewTopic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheme = get_scheme(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "coll_name='WindPropLacqOneDayP25B1000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = mongodb_client[db_name]\n",
    "scheme_collection = db['schemes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheme_doc = { \"name\" : \"WindProp\" , \"value\" : scheme }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheme_id = scheme_collection.insert_one(scheme_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertOneResult at 0x7f2cde9afdc0>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scheme_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONNECTION KAFKA :  localhost:9092\n",
      "TOPIC            :  WindPropDataLacqOneDayP25NewTopic\n",
      "DB NAME          :  TimeSeriesBench\n",
      "COLLECTION NAME  :  WindPropLacqOneDayP25B1000\n",
      "RANGE: TopicPartition(topic='WindPropDataLacqOneDayP25NewTopic', partition=0) 7288\n",
      "CPU times: user 479 ms, sys: 9.01 ms, total: 488 ms\n",
      "Wall time: 809 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bucket_size = 1000\n",
    "validate_data = False\n",
    "consumer_single_collection(kafka_server,topic,db_name, coll_name,bucket_size,mongodb_insert_data,mongodb_convert_to_dbdata,validate_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONNECTION KAFKA :  localhost:9092\n",
      "TOPIC            :  WindPropDataLacqOneDayP25New2Topic\n",
      "DB NAME          :  TimeSeriesBench\n",
      "COLLECTION NAME  :  WindPropLacqOneDayP25B1000\n",
      "RANGE: TopicPartition(topic='WindPropDataLacqOneDayP25New2Topic', partition=0) 14576\n",
      "CPU times: user 553 ms, sys: 12 ms, total: 565 ms\n",
      "Wall time: 3.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bucket_size = 1000\n",
    "validate_data = False\n",
    "consumer_single_collection(kafka_server,topic,db_name, coll_name,bucket_size,kairosdb_insert_data,db_convert_to_dbdata,validate_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONNECTION KAFKA :  localhost:9092\n",
      "TOPIC            :  WindPropDataLacqOneDayP25New2Topic\n",
      "DB NAME          :  TimeSeriesBench\n",
      "COLLECTION NAME  :  WindPropLacqOneDayP25B1000\n",
      "RANGE: TopicPartition(topic='WindPropDataLacqOneDayP25New2Topic', partition=0) 14576\n",
      "CPU times: user 856 ms, sys: 12 ms, total: 868 ms\n",
      "Wall time: 2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bucket_size = 1000\n",
    "validate_data = False\n",
    "consumer_single_collection(kafka_server,topic,db_name, coll_name,bucket_size,influxdb_insert_data,db_convert_to_dbdata,validate_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONNECTION KAFKA :  localhost:9092\n",
      "TOPIC            :  WindPropDataLacqOneDayP25New2Topic\n",
      "DB NAME          :  TimeSeriesBench\n",
      "COLLECTION NAME  :  WindPropLacqOneDayP25B1000\n",
      "RANGE: TopicPartition(topic='WindPropDataLacqOneDayP25New2Topic', partition=0) 14576\n",
      "CPU times: user 1.06 s, sys: 11.1 ms, total: 1.07 s\n",
      "Wall time: 3.37 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bucket_size = 1000\n",
    "validate_data = False\n",
    "consumer_single_collection(kafka_server,topic,db_name, coll_name,bucket_size,warp10_insert_data,db_convert_to_dbdata,validate_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
