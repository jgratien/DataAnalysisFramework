{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kafka Data Consumer\n",
    "\n",
    "## I/ Objectifs du notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II/ Mécanisme de collecte en streaming des données\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "sys.path.append(\"../TimeSeriesTools\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_server ='localhost:9092'\n",
    "#kafka_server ='islin-hdpnod1:6667'\n",
    "#kafka_server ='islin-hdplnod06:6667'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kairosdb_utils\n",
    "global kairosdb_server \n",
    "kairosdb_server = \"http://localhost:9080\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to connect to InfluxDB server without proxy: localhost on port: 8086\n",
      "connection sucess!\n"
     ]
    }
   ],
   "source": [
    "import influxdb_utils\n",
    "db_host= 'localhost'\n",
    "port = '8086'\n",
    "db_name='TimeSeriesBench'\n",
    "influxdb_client = influxdb_utils.influxdb_connect(db_host, port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer, TopicPartition\n",
    "def consumer_single_collection(kafka_server,topic,db_name, coll_name,bucket_size,insert_data,convert_to_dbdata):\n",
    "    print(\"CONNECTION KAFKA : \",kafka_server)\n",
    "    print(\"TOPIC            : \",topic)\n",
    "    print(\"DB NAME          : \",db_name)\n",
    "    print(\"COLLECTION NAME  : \",coll_name)\n",
    "    \n",
    "    scheme = get_scheme(topic)\n",
    "    consumer = KafkaConsumer(bootstrap_servers=kafka_server)\n",
    "\n",
    "    tp = TopicPartition(topic, 0)\n",
    "    consumer.assign([tp])\n",
    "    consumer.seek_to_end(tp)\n",
    "    lastOffset = consumer.position(tp)\n",
    "    print(\"RANGE:\",tp,lastOffset)\n",
    "    consumer.seek_to_beginning(tp)\n",
    "    # end\n",
    "    try :\n",
    "        while True:\n",
    "            list_data = []\n",
    "            count=0\n",
    "            for message in consumer:\n",
    "                msg = message.value.decode()\n",
    "                count += 1\n",
    "                list_data.append(msg)\n",
    "                if len(list_data) == bucket_size:\n",
    "                    print(count,str(len(list_data)) + \" data fetched from topic : \" + topic)\n",
    "                    new_data = convert_to_dbdata(list_data,scheme)\n",
    "                    insert_data(db_name, coll_name, new_data)\n",
    "                    list_data.clear()\n",
    "                if count == lastOffset:\n",
    "                    print(count,str(len(list_data)) + \" data fetched from topic : \" + topic)\n",
    "                    new_data = convert_to_dbdata(list_data,scheme)\n",
    "                    insert_data(db_name, coll_name, new_data)\n",
    "                    list_data.clear()\n",
    "                    return\n",
    "\n",
    "    finally:\n",
    "        # Close down consumer to commit final offsets.\n",
    "        consumer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III/ Mécanismes de validation et corrections des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scheme(topic):\n",
    "    if \"SmartGrid\" in topic:\n",
    "        scheme = {\"timestamp\":{'type': 'string','required': True,'empty': False},\n",
    "                  \"tagname\":{'type': 'string','required': True,'empty': False},\n",
    "                  \"value\":{'type': 'string','required': True,'empty': False},\n",
    "                  \"quality\":{'type': 'string','required': True,'empty': False}}\n",
    "        return scheme\n",
    "    if \"WindProp\" in topic :\n",
    "        scheme = {  \"Heure\":{'type': 'string','required': True,'empty': False},\n",
    "                    \"Temps écoulé\":{'type': 'string','required': True,'empty': False},\n",
    "                    '4069 state':{'type': 'string','required': True,'empty': False},\n",
    "                    'Battery voltage':{'type': 'string','required': True,'empty': False},\n",
    "                    'Flow SP':{'type': 'string','required': True,'empty': False},\n",
    "                    'Unit Code':{'type': 'string','required': True,'empty': False},\n",
    "                    'Flow M':{'type': 'string','required': True,'empty': False},\n",
    "                    'Pressure1':{'type': 'string','required': True,'empty': False},\n",
    "                    'Temperature1':{'type': 'string','required': True,'empty': False},\n",
    "                    \"Latitude\":{'type': 'string','required': True,'empty': False},\n",
    "                    \"Longitude\":{'type': 'string','required': True,'empty': False},\n",
    "                    \"Altitude\":{'type': 'string','required': True,'empty': False},\n",
    "                    \"Head_Rel_True North\":{'type': 'string','required': True,'empty': False},\n",
    "                    \"Pressure2\":{'type': 'string','required': True,'empty': False},\n",
    "                    \"Temperature2\":{'type': 'string','required': True,'empty': False},\n",
    "                    \"Humidity\":{'type': 'string','required': True,'empty': False},\n",
    "                    \"MDA Wnd Dir\":{'type': 'string','required': True,'empty': False},\n",
    "                    \"MDA Wnd Speed\":{'type': 'string','required': True,'empty': False},\n",
    "                    \"MWD Wind Dir\":{'type': 'string','required': True,'empty': False},\n",
    "                    \"MWD Wind Speed\":{'type': 'string','required': True,'empty': False},\n",
    "                    'Gaz Concentration 1':{'type': 'string','required': True,'empty': False},\n",
    "                    'Gaz Concentration 2':{'type': 'string','required': True,'empty': False},\n",
    "                    'Gaz Concentration 3':{'type': 'string','required': True,'empty': False},\n",
    "                    'Gaz Concentration 4':{'type': 'string','required': True,'empty': False},\n",
    "                    'SPA 1':{'type': 'string','required': True,'empty': False},\n",
    "                    'SPA 2':{'type': 'string','required': True,'empty': False},\n",
    "                    'Cellule Photo':{'type': 'string','required': True,'empty': False},\n",
    "                    'Temperature3':{'type': 'string','required': True,'empty': False},\n",
    "                    'Pressure3':{'type': 'string','required': True,'empty': False},\n",
    "                    'Flow MassFlow 1':{'type': 'string','required': True,'empty': False},\n",
    "                    'NOTUSED Flow MassFlow 2':{'type': 'string','required': True,'empty': False},\n",
    "                    'Flow':{'type': 'string','required': True,'empty': False},\n",
    "                    'Humidity2':{'type': 'string','required': True,'empty': False},\n",
    "                    'Test':{'type': 'string','required': True,'empty': False},\n",
    "                    'Details':{'type': 'string','required': True,'empty': False},\n",
    "                    'SPA 3':{'type': 'string','required': True,'empty': False},\n",
    "                    'SPA 4':{'type': 'string','required': True,'empty': False},\n",
    "                    \"CavityPressure\":{'type': 'string','required': True,'empty': False},\n",
    "                    \"CavityTemp\":{'type': 'string','required': True,'empty': False},\n",
    "                    \"CH4\":{'type': 'string','required': True,'empty': False},\n",
    "                    \"CH4_dry\":{'type': 'string','required': True,'empty': False},\n",
    "                    \"C2H6\":{'type': 'string','required': True,'empty': False},\n",
    "                    \"C2H6_dry\":{'type': 'string','required': True,'empty': False},\n",
    "                    \"13CH4\":{'type': 'string','required': True,'empty': False},\n",
    "                    \"H2O\":{'type': 'string','required': True,'empty': False},\n",
    "                    \"CO2\":{'type': 'string','required': True,'empty': False},\n",
    "                    \"C2C1Ratio\":{'type': 'string','required': True,'empty': False},\n",
    "                    \"Delta_iCH4_Raw\":{'type': 'string','required': True,'empty': False},\n",
    "                    \"HP_Delta_iCH4_30s\":{'type': 'string','required': True,'empty': False},\n",
    "                    \"HP_Delta_iCH4_2min\":{'type': 'string','required': True,'empty': False},\n",
    "                    \"HP_Delta_iCH4_5min\":{'type': 'string','required': True,'empty': False} }\n",
    "        return scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data,scheme):\n",
    "    from cerberus import Validator\n",
    "    v = Validator(scheme)\n",
    "    for index,item in enumerate(data,start=0):\n",
    "        res = v.validate(item)\n",
    "        if (res == False):\n",
    "            print(\"corrupt data in line :\",index,\", error : \",v.errors)\n",
    "            del data[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV/ Mécanismes de mise en base des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to connect to MongoDB server: localhost on port: 27018\n"
     ]
    }
   ],
   "source": [
    "import mongodb_utils\n",
    "db_host= 'localhost'\n",
    "port = '27018'\n",
    "db_name='TimeSeriesBench'\n",
    "mongodb_client = mongodb_utils.mongodb_connect(db_host, port)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A/ Mise en base du schéma des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_unix(date):\n",
    "    from datetime import datetime\n",
    "    dt = datetime.strptime(date, '%d/%m/%Y %H:%M:%S')\n",
    "    epoch = datetime.utcfromtimestamp(0)\n",
    "    return int((dt - epoch).total_seconds()) * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def type_convert(data):\n",
    "    data = data.replace(',', '.')\n",
    "    time = str_to_unix(data.split(\";\", 1)[0].split(\":\", 1)[1])\n",
    "    tags = data.split(\";\", 2)[1]\n",
    "    data_treated = [tags,[time, data.split(\";\", 2)[2]]]\n",
    "    return data_treated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mongodb_convert_to_dbdata(list_data,scheme):\n",
    "    col= [k for k in scheme.keys()]\n",
    "    new_dict=[]\n",
    "    \n",
    "    t0 = time.process_time()\n",
    "    for item in list_data:\n",
    "        info=item.split(\":\",1)[1].split(\";\")\n",
    "        dict_item ={col[i]:info[i] for i in range(len(col))}\n",
    "        new_dict.append(dict_item)\n",
    "    #clean_data(new_dict,scheme)\n",
    "    t1 = time.process_time()\n",
    "    #print('.. %f seconds for validate' % (t1 - t0))\n",
    "    \n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def db_convert_to_dbdata(list_msg,scheme):\n",
    "    from cerberus import Validator\n",
    "    col = [k for k in scheme.keys()]\n",
    "    #v = Validator(scheme)\n",
    "    db_data = []\n",
    "    t0 = time.process_time()\n",
    "    for index,item in enumerate(list_msg):\n",
    "        #info = item.split(\":\", 1)[1].split(\";\")\n",
    "        #dict_item = {col[i]: info[i] for i in range(len(col))}\n",
    "        #res = v.validate(dict_item)\n",
    "        res = True\n",
    "        if res == True:\n",
    "            new_data = type_convert(item)\n",
    "            db_data.append(new_data)\n",
    "        else:\n",
    "            print(\"corrupt data in line :\",index,\", error : \",v.errors)\n",
    "    t1 = time.process_time()\n",
    "    #print('.. %f seconds for validate' % (t1 - t0))\n",
    "    return db_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mongodb_insert_data(db_name, coll_name, list_data):\n",
    "    mongodb_utils.insert_many_docs(mongodb_client,db_name, coll_name,list_data)    \n",
    "    return len(list_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kairosdb_insert_data(db_name, coll_name, list_data):\n",
    "    tags_data_list = {}\n",
    "    for d in list_data:\n",
    "        if d[0] in tags_data_list.keys():\n",
    "            tags_data_list[d[0]].append(d[1])\n",
    "        else:\n",
    "            tags_data_list[d[0]] = [d[1]]\n",
    "    \n",
    "    tag_keys = ['Buiding','Device','Measure']\n",
    "    for i,(k,v) in enumerate(tags_data_list.items()):\n",
    "        tag_values = k.split(\".\")\n",
    "        tags = {tag_keys[i]:tag_values[i] for i in range(len(tag_keys))}\n",
    "        kairosdb_utils.insert_many_docs_with_tags(kairosdb_server,db_name, coll_name,tags,v)    \n",
    "    return len(list_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def influxdb_insert_data(db_name, coll_name, list_data):\n",
    "    tags_data_list = {}\n",
    "    for d in list_data:\n",
    "        if d[0] in tags_data_list.keys():\n",
    "            tags_data_list[d[0]].append(d[1])\n",
    "        else:\n",
    "            tags_data_list[d[0]] = [d[1]]\n",
    "    \n",
    "    tag_keys = ['Buiding','Device','Measure']\n",
    "    for i,(k,v) in enumerate(tags_data_list.items()):\n",
    "        tag_values = k.split(\".\")\n",
    "        tags = {tag_keys[i]:tag_values[i] for i in range(len(tag_keys))}\n",
    "        influxdb_utils.insert_many_docs_with_tags(influxdb_client,db_name, coll_name,tags,v)    \n",
    "    return len(list_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Cas test SmartGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = 'SmartGridDataCryolite20190101OneMonthTopic'\n",
    "scheme = get_scheme(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db=mongodb_client[db_name]\n",
    "db.list_collection_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheme_collection = db['schemes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheme_doc = { \"name\" : \"SmartGrid\" , \"value\" : scheme }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheme_id = scheme_collection.insert_one(scheme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheme_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "coll_name='SmartGridCryolite20190101OneMonthV10000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "bucket_size = 10000\n",
    "consumer_single_collection(kafka_server,topic,db_name, coll_name,bucket_size,mongodb_insert_data,mongodb_convert_to_dbdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "bucket_size = 10000\n",
    "consumer_single_collection(kafka_server,topic,db_name, coll_name,bucket_size,kairosdb_insert_data,db_convert_to_dbdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONNECTION KAFKA :  localhost:9092\n",
      "TOPIC            :  SmartGridDataCryolite20190101OneMonthTopic\n",
      "DB NAME          :  TimeSeriesBench\n",
      "COLLECTION NAME  :  SmartGridCryolite20190101OneMonthV10000\n",
      "RANGE: TopicPartition(topic='SmartGridDataCryolite20190101OneMonthTopic', partition=0) 769598\n",
      "10000 10000 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "10000 documents have been inserted\n",
      "20000 10000 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "10000 documents have been inserted\n",
      "30000 10000 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "7584 documents have been inserted\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "2416 documents have been inserted\n",
      "40000 10000 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "10000 documents have been inserted\n",
      "50000 10000 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "10000 documents have been inserted\n",
      "60000 10000 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "1652 documents have been inserted\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "8348 documents have been inserted\n",
      "70000 10000 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "9495 documents have been inserted\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "505 documents have been inserted\n",
      "80000 10000 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "10000 documents have been inserted\n",
      "90000 10000 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "10000 documents have been inserted\n",
      "100000 10000 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "10000 documents have been inserted\n",
      "110000 10000 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "10000 documents have been inserted\n",
      "120000 10000 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "10000 documents have been inserted\n",
      "130000 10000 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "10000 documents have been inserted\n",
      "140000 10000 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "10000 documents have been inserted\n",
      "150000 10000 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "10000 documents have been inserted\n",
      "160000 10000 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "10000 documents have been inserted\n",
      "170000 10000 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "10000 documents have been inserted\n",
      "180000 10000 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "10000 documents have been inserted\n",
      "190000 10000 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "10000 documents have been inserted\n",
      "200000 10000 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "10000 documents have been inserted\n",
      "210000 10000 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "10000 documents have been inserted\n",
      "220000 10000 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "10000 documents have been inserted\n",
      "230000 10000 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "10000 documents have been inserted\n",
      "240000 10000 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "10000 documents have been inserted\n",
      "250000 10000 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "10000 documents have been inserted\n",
      "260000 10000 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "10000 documents have been inserted\n",
      "270000 10000 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "10000 documents have been inserted\n",
      "280000 10000 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "10000 documents have been inserted\n",
      "290000 10000 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "10000 documents have been inserted\n",
      "300000 10000 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "10000 documents have been inserted\n",
      "310000 10000 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "10000 documents have been inserted\n",
      "320000 10000 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "10000 documents have been inserted\n",
      "330000 10000 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "10000 documents have been inserted\n",
      "340000 10000 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "5328 documents have been inserted\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "4672 documents have been inserted\n",
      "350000 10000 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "10000 documents have been inserted\n",
      "360000 10000 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "10000 documents have been inserted\n",
      "370000 10000 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "10000 documents have been inserted\n",
      "380000 10000 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "10000 documents have been inserted\n",
      "390000 10000 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "10000 documents have been inserted\n",
      "400000 10000 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "10000 documents have been inserted\n",
      "410000 10000 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "10000 documents have been inserted\n",
      "420000 10000 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "10000 documents have been inserted\n",
      "430000 10000 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "10000 documents have been inserted\n",
      "440000 10000 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "10000 documents have been inserted\n",
      "450000 10000 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "10000 documents have been inserted\n",
      "460000 10000 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "10000 documents have been inserted\n",
      "470000 10000 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "10000 documents have been inserted\n",
      "480000 10000 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "10000 documents have been inserted\n",
      "490000 10000 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "10000 documents have been inserted\n",
      "500000 10000 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "10000 documents have been inserted\n",
      "510000 10000 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "10000 documents have been inserted\n",
      "520000 10000 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "10000 documents have been inserted\n",
      "530000 10000 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "10000 documents have been inserted\n",
      "540000 10000 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "10000 documents have been inserted\n",
      "550000 10000 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "10000 documents have been inserted\n",
      "560000 10000 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "10000 documents have been inserted\n",
      "570000 10000 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "10000 documents have been inserted\n",
      "580000 10000 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "10000 documents have been inserted\n",
      "590000 10000 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "10000 documents have been inserted\n",
      "600000 10000 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "10000 documents have been inserted\n",
      "610000 10000 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "3515 documents have been inserted\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "6485 documents have been inserted\n",
      "620000 10000 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "6338 documents have been inserted\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "3662 documents have been inserted\n",
      "630000 10000 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "10000 documents have been inserted\n",
      "640000 10000 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "10000 documents have been inserted\n",
      "650000 10000 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "10000 documents have been inserted\n",
      "660000 10000 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "10000 documents have been inserted\n",
      "670000 10000 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "10000 documents have been inserted\n",
      "680000 10000 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "10000 documents have been inserted\n",
      "690000 10000 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "10000 documents have been inserted\n",
      "700000 10000 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "10000 documents have been inserted\n",
      "710000 10000 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "10000 documents have been inserted\n",
      "720000 10000 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "10000 documents have been inserted\n",
      "730000 10000 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "10000 documents have been inserted\n",
      "740000 10000 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "10000 documents have been inserted\n",
      "750000 10000 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "10000 documents have been inserted\n",
      "760000 10000 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "10000 documents have been inserted\n",
      "769598 9598 data fetched from topic : SmartGridDataCryolite20190101OneMonthTopic\n",
      "TimeSeriesBench _internal\n",
      "TimeSeriesBench TimeSeriesBench\n",
      "9598 documents have been inserted\n",
      "CPU times: user 39.1 s, sys: 663 ms, total: 39.8 s\n",
      "Wall time: 48.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bucket_size = 10000\n",
    "consumer_single_collection(kafka_server,topic,db_name, coll_name,bucket_size,influxdb_insert_data,db_convert_to_dbdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Cas test WindProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def type_convert(data):\n",
    "    data = data.replace(',', '.')\n",
    "    time = str_to_unix(data.split(\";\", 1)[0].split(\":\", 1)[1])\n",
    "    data_treated = [time, data.split(\";\", 1)[1]]\n",
    "    return data_treated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def db_convert_to_dbdata(list_msg,scheme):\n",
    "    from cerberus import Validator\n",
    "    col = [k for k in scheme.keys()]\n",
    "    #v = Validator(scheme)\n",
    "    db_data = []\n",
    "    t0 = time.process_time()\n",
    "    for index,item in enumerate(list_msg):\n",
    "        #info = item.split(\":\", 1)[1].split(\";\")\n",
    "        #dict_item = {col[i]: info[i] for i in range(len(col))}\n",
    "        #res = v.validate(dict_item)\n",
    "        res = True\n",
    "        if res == True:\n",
    "            new_data = type_convert(item)\n",
    "            db_data.append(new_data)\n",
    "        else:\n",
    "            print(\"corrupt data in line :\",index,\", error : \",v.errors)\n",
    "    t1 = time.process_time()\n",
    "    #print('.. %f seconds for validate' % (t1 - t0))\n",
    "    return db_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kairosdb_insert_data(db_name, coll_name, list_data):\n",
    "    kairosdb_utils.insert_many_docs(kairosdb_server,db_name, coll_name,list_data)    \n",
    "    return len(list_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def influxdb_insert_data(db_name, coll_name, list_data):\n",
    "    influxdb_utils.insert_many_docs(influxdb_client,db_name, coll_name,list_data)    \n",
    "    return len(list_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = 'WindPropDataLacqOneDayP25NewTopic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheme = get_scheme(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = mongodb_client[db_name]\n",
    "scheme_collection = db['schemes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheme_id = scheme_collection.insert_one(scheme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheme_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coll_name='WindPropLacqOneDayP25-B1000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "bucket_size = 1000\n",
    "consumer_single_collection(kafka_server,topic,db_name, coll_name,bucket_size,mongodb_insert_data,mongodb_convert_to_dbdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "bucket_size = 1000\n",
    "consumer_single_collection(kafka_server,topic,db_name, coll_name,bucket_size,kairosdb_insert_data,db_convert_to_dbdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "bucket_size = 1000\n",
    "consumer_single_collection(kafka_server,topic,db_name, coll_name,bucket_size,influxdb_insert_data,db_convert_to_dbdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Cas test Pollution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
